Overall, Lab 2 part 1 was about implementing Aggregates and Lab 2 part 2 was about implementing inserts, deletes, and the cache eviction policy.Filter	Implementing Filter was simple because we simply compared the values of the fields depending on the predicate. There were no real design decisions for this portion of the lab. However, implementing Join turned to be rather tricky.Join	For join, I decided to use a hash-join for equal-joins, and nested-loop joins for all other predicates. The reason for this is, hash-joins have an IO cost of Pages(Relation A) + Pages(Relation B), while page-nested-loop joins have IO cost of Pages(Relation A) + (Pages(Relation A) * Pages(Relation B)), which is significantly higher. Hash-joins are especially good for equality checks because we simply need to check if a value exists in the table. If the predicate is some sort of ranged check (such as less than), then we will need to iterate through the whole hash-table, which makes the hash-table a lot less efficient than sorted data structures such as binary tress.	Overall, nested-loop joins were the simplest to implement, but have horrible run times. Hash-joins have extremely good performance if one of the relations could entirely fit in memory, and if the predicate is EQUALS. In order to avoid the horrid performance of ranged-comparison from hash-joins, I used nested-loop joins for everything except for equal-joins where I used a hash-join.Integer Aggregator and String Aggregator	Designing these two Aggregators proved to be rather difficult. First, I thought about making subclasses because these classes must handle grouping/non-grouping, and numerous operations. Each combination resulted in the usage of a different data structure. However, I decided against this idea for the sake of time. The way these classes are currently organized is not good because it is hard to add additional operations to support. If an additional operation were to be added, new cases statements and if statements would be needed to be written in several places, while if there were subclasses, we could create a new sub class. Other than that, everything was fairly simple. I decided to use HashMaps for grouping because each Group would return only 1 result. The GroupBy attribute would be the key, and the return value would be stored as the key.Aggregate	This class was simple to implement. Depending on the input parameters, we used an Integer Aggregator or a String Aggregator under the hood to aggregate our values. We then used those respective iterators to iterate through the dataset.Insert and Delete	Insert and Delete’s getTupleDesc was very confusing. Up until now, we would always return the DbIterator’s TupleDesc, but here, we hard code a TupleDesc that contains a single IntField. It took me a while to figure out that this is what the test code wanted. Other than that, we insert and delete through BufferPool’s insert and delete methods.Heap Page Heap File	Insert tuple for these two classes were really simple. Heap Page simply checked to see if it had any open slots, and inserted to the Tuple if there was one. Since the Heap Page should already be in Buffer Pool, there is no need to actually write the Tuple to disk because the whole page will be written when the Heap Page is evicted from Buffer Pool. This was the same for delete tuple in Heap Page.	Heap File's insertTuple was also simple. I iterated through all of it’s heap pages, and inserted using HeapPage’s insert tuple. If there were no open slots in any of the current pages, I would write a new page to disk, call on it through Buffer Pool, then use Heap Page’s insert/delete.Buffer Pool Insert/Delete	Buffer Pool’s insert simply used Heap File’s insertTuple. Buffer Pool then took the returned list of dirty pages, marked it dirty, then checked the Buffer Pool to see if the same page was already in the cache. If it was, it would replace the new dirty page with the old page. If the old page was dirty, it would flush it, evict it, then replace it. If there were no old copies, it would insert it into the buffer pool. If there were no more open slots, it would evict a page, then insert it in the newly opened slot.	As long as the dirty pages are written to buffer pool at some point of time, it will eventually be evicted, which will in turn be flushed to disk. Therefore, as long as it finds its way to buffer pool, there is no reason for us to ever manually write dirty pages to disk. That is Buffer Pool’s job. Buffer Pool Eviction	I decided to implement a random eviction policy for my buffer pool. Random eviction is much better than fixed index eviction because fixed index evictions always evict newly inserted pages. Since we generally strive for temporal efficiency, it is not a good idea to evict newly read pages. Although LRU eviction would be better for the reason above, I implemented random eviction because it was the simplest policy that would yield reasonable performance.The most confusing part of this lab was what we needed to do with the returned list of dirty pages in buffer pool's insert and delete tuple. It was written in the assignment, that whenever we insert/delete in Heap Page and Heap File, we were to use Buffer Pool's getPage so that the pages would always be in our Buffer Pool. Since we always call Heap Page and Heap File's insert/delete methods, I would actually like to have marked the pages dirty then, instead of in Buffer Pool. No matter who calls insert/delete, Heap Page and Heap File should always mark the respective pages dirty.Secondly, the Heap Pages should always be in Buffer Pool first. However, in the test suite for test multiple dirty pages, the test code extends our Heap File and overrides our insertTuple. However, IT DOES NOT USE BUFFER POOL'S GETPAGE, so the newly constructed pages were never in Buffer Pool. This caused my code many problems, but I eventually solved it by manually inserting every dirty page returned to me after insert/delete page directly into buffer pool. This really should have been explicity specified. This required me to read through all of the test code and understand exactly what it was that the unit tests were doing.\Overall, this assignment took me about 30 hours.Query 1: 0.17 secondsQuery 2: 0.93 secondsQuery 3: 1.65 seconds